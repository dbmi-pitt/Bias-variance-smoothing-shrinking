---
output: 
  html_document:
runtime: shiny
---

```{r listings}
source('initializations.R', local=TRUE)
# ls()
# cat('-=-=-=-=\n')
# ls(pos=1)
# cat('-=-=-=-=\n')
# so it has its own .GlobalEnv, and it's persistent across knits.
# wrapperToGetKeys  ### interesting! that's mine! from DebugTools
```



```{r hide=TRUE}
require('shiny')
```

##	"Lumping" versus "splitting" in science. 

In science, advances often proceed by "splitting": things that seem initially the same are classified as different. The classification may be tentativedistinction, until it proves to be useful. 
Other advances proceed by "lumping":  identifying things that seem different but have a deep connection.

So, we split birds and bats, but we lump bats and whales because they are both mammals. At least in this case, for some purposes, lumping supercedes splitting.
(But if studying modes of  locomotion, the lumping will be different.)

In medicine both splitting and lumping are important.

Cancer was just "cancer" before it was learned through clinical trials that some medicines work well on cancer in one organ but not another--- and vice versa for another medicine.

In recent years, however, specific molecular defects in cancer cells have "lumped" together types of cancer that we would not have dreamed of connecting.
The drug gleevec is a miracle drug for patients with chronic myelogenous leukemia (CML). But not for other leukemias. Deep insight into molecular biology of cancer showed that a radically different kind, gastrointestinal stromal tumor (GIST). 
Better lumping together with better splitting can cure cancer patients.

We will explore a simple lumping and splitting conundrum as a jumping off point for introducing a collection of important statistical ideas that connect to each other.

```{r}
QandAha()
```

### Hypothetical medical study

The Problem:  A new treatment is given to 100 patients.  
Of them, only 8 respond.  But there is a subgroup of 5 in which 3 patients respond, yielding a response rate of 60%. for now we call them "D people", and the others "L people".

Goal: decide whether to treat the "D" patients (or at least, whether to put resources into studying it further for "D" patients).

If you *"lump"*, you will estimate that the response rate (`Pr(R|D)`) for D people is *8%*. 

If you *"split"*, you will estimate that the response rate (`Pr(R|D)`) for D people is *60%*. 

```{r plotLumpSplitPoints, fig.height=300}
output$plotLumpSplitPoints = renderPlot(
  height = 300, {
  plotPlightPdarkPosterior(DLdata=rValues$DLdata,
                           showPrior = FALSE, showPosterior=FALSE, showLikelihood=FALSE,
  showS = TRUE,
  showL = TRUE,
  showW = FALSE)
})
fluidRow(column(6,
                h3(style='color:red;', "L = Lump, S = Split"),
                plotOutput('plotLumpSplitPoints')),
         column(6,
                dataTableComponent())
)
```

Lumping gives an answer with low variance (N=100) but high bias-- because the answer reflects far more L  people than D people. The confidence interval follows the diagonal where $Pr(R|D)=Pr(R|L)$.

Splitting gives an answer with high variance (N=5) but low bias-- because it is including only people who ARE D:  it is directly asking our question. The very wide horizontal confidence interval reflects this high variance.
To view the data, click here:

```{r}
(interactiveSection(
  codeToRun = "updateCheckboxInput(session=thisSession, inputId='toggleShowData', value=TRUE)", 
  buttonLabel= "show data")
)
```

We will call the patient "feature" $X$. Its values is D or L.
The identity of $X$ will be explored later.

The outcome we call $Y$.  A responding patient has $Y=R$; non-reponding, $Y=N$.

#### Parametrizations of the probabilities

```{r child = 'Rmd-text-snippets/parametrize.Rmd'}
```

```{r}
#includeHTML('Rmd-text-snippets/parametrize.html')
#inclRmd('Rmd-text-snippets/parametrize.Rmd')
TextQuestion('Why focus on this parameter?')
```


```{r QA2}
QandAha()
```

#### Approaches

* [Classical test: is X independent of the outcome Y?](#Classical test)
* [Bayes mixture:  "Dr. Who"](#Bayes_mixture)
* [Bayes joint prior, logit scale](#Bayes_joint_prior).  
* Approaches used in the famous ECMO data set. (ToDo)

##### <a name='Classical test'> Classical estimation bifurcating on a hypothesis test</a>

A time-honored but fading technique is to perform a classical hypothesis test.
Here the hypothesis is "X is independent of Y";  the response rate of D people does not differ from L people.  Then, depending on the hypothesis test result, one answer or the other is selected. 

A Fisher exact test is frequently performed to test independence in a 2x2 table like this one.

```{r fisher}
interactiveSection(
  codeToRun = "cat('P = ', fisher.test(DLdata)$p.value, '\n')",
  buttonLabel= "run fishertest")
```

If the P-value is significant, the "null hypothesis" (that "Dr. Lump" is correct) is rejected. With the original data, the traditional consequence is that we use Dr. Split's estimate:  60%.
```{r}
QandAha()
```

##### <a name='Bayes_mixture'> Approach: Bayes mixture of "Lump" and "Split"</a>

###### Dr. Split speaks:

```{r child = 'DrSplit.Rmd'}
```
###### Dr. Lump disagrees!  

```{r child = 'DrLump.Rmd'}
```

###### Dr. Who doesn't know *who* to believe:


```{r child = 'DrWho.Rmd'}
```



```{r QA3}
QandAha()
```

##### <a name='Bayes_joint_prior'> Approach: Bayes joint prior, logit scale.</a>

Another Bayesian approach is to set a Bayesian prior on the joint distribution of the two conditional probabilities Pr(R|D) and Pr(R|L). This allows data on the two probabilities to be shared between them, smoothly instead of a crude mixture of the two extreme views of   "Dr. Lump" and "Dr. Split".
```{r}
QandAha()
```
Once we have this joint prior distribution, we compute the joint posterior.
```{r QA4}
QandAha()
```
To set the prior on the joint distribution of Pr(R|D) and Pr(R|L), we first convert them to logits ("logit" means "log(odds)"; details <a href='www/logit-info.htm').
Then we set a bivariate normal distribution on the logits.

We also convert the observed proportions to logits. We estimate the variances using the delta method applied to the Poisson distribution, whereby the variance of the logarithm of a count is roughly the reciprocal of the count.

(This doesn't work well if the count is near zero, and not at all at zero.
For this reason a "continuity fudge" is applied, if necessary.)

Finally, Pr(R|D) marginalized over Pr(R|L) is:



The link to bivariate normal details is here:

```{r linkoutbivariate}
  interactiveSection(codeToRun = 
linkout('./lump,split-bivariate-normal-derivation.pdf'))
```

#####  Approach: Optimizing the mixture with cross-validation

Cross-validation is a technique in which the performance of a model can be checked as if on an independent dataset, without actually having a separate dataset. `K-fold cross-validation` divides the dataset into K disjoint subsets of observations, and loops over these sets. Each subset is, in turn, used as a `test set`. We remove it from the full dataset, re-build the model on the reduced dataset (the `training set`), and evaluate predictions of that model on the removed observations in the test set. Performance metrics are usually averaged across the K repetitions. When $K=N$, the size of the full dataset, the procedure is called `leave-one-out cross-validation`, since each test set is of size one.

For this example dataset, we perform leave-one-out cross-validation as follows:

```{r}
source('Lump-Split-crossvalidation.R', local=TRUE, echo = FALSE)
```

#### Summary of approaches 

Assembling the results of the approaches to estimating Pr(R|D):

```{r}
### ToDo
data.frame(Lump=0.08, Split=0.60, Who=0.57, CV=0.47, 
           BayesLogit=NA)
### ToDo extract the point estimate from BayesLogit.
```

###  Save your Comments and Answers
```{r assembleComments}
assembleComments <<- function() {
  lastQANumber = nextNumber(sequenceType = "QA") - 1
  answers = sapply(1:lastQANumber, function(num) {
    outputIdThisQA = paste0('QA', num)
    textareaIdThisQA = paste0('id', outputIdThisQA)
    return(input[[textareaIdThisQA]])
  })
  unlist(answers[-which(sapply(answers, is.null))])
}
saveComments <<- function() {
  answers = assembleComments()
  writeLines(answers, file(paste0('answers.',
                                 Sys.info()["user"],
                                 '.txt'))
  )
}

interactiveSection(codeToRun = "saveComments()")
```

# Other topics to be developed:
##	The famous ECMO data set.
##	The role of prior information, explored by introducing different identities for "D" and "L".
###	2 alleles of a gene known to affect this drug's pharmacodynamics
###	2 alleles of one gene out of a hundred tested, chosen because they are known to affect this drug's pharmacodynamics
###	2 alleles of one gene out of a hundred thousand tested; nothing known
###	D = dark hair, L = light hair
###	D = dark hair, L = light hair; but hair color is strongly tied to ethnicityâ€¦ which is strongly tied to a key genetic variant
##	Curse of dimensionality,
##	Multiple testing and estimation with high-dimensional priors,


###### Other navigation links
## -> Top of the [Notebook](#notebookBegin) .
## -> [Debugging panel](#debugging) .
## -> [Code for the Plot](#plotCode).
## -> [viewPlot](#viewPlot).



