---
runtime: shiny
output: 
  html_notebook: 
    number_sections: yes
    toc: yes
---

```{r listings}
# ls()
# cat('-=-=-=-=\n')
# ls(pos=1)
# cat('-=-=-=-=\n')
# so it has its own .GlobalEnv, and it's persistent across knits.
# wrapperToGetKeys  ### interesting! that's mine! from DebugTools
```



```{r hide=TRUE}
require('shiny')
#inclRmd('lumpsplit.Rmd')
## Yes, you can have 2 inclRmd in one file.
## But this one busted the shiny responses.
```

#### Introduction to this sub-module

In science, advances often proceed by "splitting": things that seem initially the same are classified as different. The classification may be tentativedistinction, until it proves to be useful. 
Other advances proceed by "lumping":  identifying things that seem different but have a deep connection.

So, we split birds and bats, but we lump bats and whales because they are both mammals. At least in this case, for some purposes, lumping supercedes splitting.
(But if studying modes of  locomotion, the lumping will be different.)

In medicine both splitting and lumping are important.

Cancer was just "cancer" before it was learned through clinical trials that some medicines work well on cancer in one organ but not another--- and vice versa for another medicine.

In recent years, however, specific molecular defects in cancer cells have "lumped" together types of cancer that we would not have dreamed of connecting.
The drug gleevec is a miracle drug for patients with chronic myelogenous leukemia (CML). But not for other leukemias. Deep insight into molecular biology of cancer showed that a radically different kind, gastrointestinal stromal tumor (GIST). 
Better lumping together with better splitting can cure cancer patients.

We will explore a simple lumping and splitting conundrum as a jumping off point for introducing a collection of important statistical ideas that connect to each other.

```{r}
QandAha()
```

### Hypothetical medical study demonstrating lumping versus splitting

The Problem:  A new treatment is given to 100 patients.  
Of them, only 8 respond.  But there is a subgroup of 5 in which 3 patients respond, yielding a response rate of 60%. for now we call them "D people", and the others "L people".

Goal: decide whether to treat the "D" patients (or at least, whether to put resources into studying it further for "D" patients).

If you *"lump"*, you will estimate that the response rate (`Pr(R|D)`) for D people is *8%*. 

If you *"split"*, you will estimate that the response rate (`Pr(R|D)`) for D people is *60%*. 

```{r plotLumpSplitPoints}
output$plotLumpSplitPoints = renderPlot({
  plotPlightPdarkPosterior(showPrior = FALSE, showPosterior=FALSE, showLikelihood=FALSE,
  showS = TRUE,
  showL = TRUE,
  showW = FALSE)
})

div(plotOutput('plotLumpSplitPoints'))
```

Lumping gives an answer with low variance (N=100) but high bias-- because the answer reflects far more L  people than D people. The confidence interval follows the diagonal where $Pr(R|D)=Pr(R|L)$.

Splitting gives an answer with high variance (N=5) but low bias-- because it is including only people who ARE D:  it is directly asking our question. The very wide horizontal confidence interval reflects this high variance.
To view the data, click here:

```{r}
(interactiveSection(
  codeToRun = "updateCheckboxInput(session=thisSession, inputId='toggleShowData', value=TRUE)", 
  buttonLabel= "show data")
)
```

We will call the patient "feature" $X$. Its values is D or L.
The identity of $X$ will be explored later.

The outcome we call $Y$.  A responding patient has $Y=R$; non-reponding, $Y=N$.

### Parametrizations of the probabilities

```{r child = 'Rmd-text-snippets/parametrize.Rmd'}
```

```{r}
#includeHTML('Rmd-text-snippets/parametrize.html')
#inclRmd('Rmd-text-snippets/parametrize.Rmd')
TextQuestion('Why focus on this parameter?')
```


```{r QA2}
QandAha()
```

### Approaches

* [Classical test: is X independent of the outcome Y?](#Classical test)
* [Bayes mixture:  "Dr. Who"](#Bayes_mixture)
* [Bayes joint prior, logit scale](#Bayes_joint_prior).  
* Approaches used in the famous ECMO data set. (ToDo)

#### <a name='Classical test'> Approach: Classical test: is X independent of the outcome Y?</a>

A time-honored but fading technique is to perform a classical hypothesis test.
Here the hypothesis is "X is independent of Y";  the response rate of D people does not differ from L people.  Then, depending on the hypothesis test result, one answer or the other is selected. 

A Fisher exact test is frequently performed to test independence in a 2x2 table like this one.

```{r fisher}
interactiveSection(
  codeToRun = "cat('P = ', fisher.test(DLdata)$p.value, '\n')",
  buttonLabel= "run fishertest")
```

Since the P-value is significant, the "null hypothesis" (that "Dr. Lump" is correct) is rejected. The traditional consequence is that we use Dr. Split's estimate:  60%.
```{r}
QandAha()
```

#### <a name='Bayes_mixture'> Approach: Bayes mixture of "Lump" and "Split"</a>

```{r child = 'DrSplit.Rmd'}
```

```{r child = 'DrLump.Rmd'}
```

```{r child = 'DrWho.Rmd'}
```


```{r QA3}
QandAha()
```

#### <a name='Bayes_joint_prior'> Approach: Bayes joint prior, logit scale.</a>

Another Bayesian approach is to set a Bayesian prior on the joint distribution of the two conditional probabilities Pr(R|D) and Pr(R|L). This allows data on the two probabilities to be shared between them, smoothly instead of a crude mixture of the two extreme views of   "Dr. Lump" and "Dr. Split".
```{r}
QandAha()
```
Once we have this joint prior distribution, we compute the joint posterior.
```{r QA4}
QandAha()
```
To set the prior on the joint distribution of Pr(R|D) and Pr(R|L), we first convert them to logits ("logit" means "log(odds)"; details <a href='www/logit-info.htm').
Then we set a bivariate normal distribution on the logits.

We also convert the observed proportions to logits. We estimate the variances using the delta method applied to the Poisson distribution, whereby the variance of the logarithm of a count is roughly the reciprocal of the count.

(This doesn't work well if the count is near zero, and not at all at zero.
For this reason a "continuity fudge" is applied, if necessary.)

The link to bivariate normal details is here:

```{r linkoutbivariate}
  interactiveSection(codeToRun = 
linkout('./lump,split-bivariate-normal-derivation.pdf'))
```

```{r assembleComments}
assembleComments <<- function() {
  lastQANumber = nextNumber(sequenceType = "QA") - 1
  answers = sapply(1:lastQANumber, function(num) {
    outputIdThisQA = paste0('QA', num)
    textareaIdThisQA = paste0('id', outputIdThisQA)
    return(input[[textareaIdThisQA]])
  })
  unlist(answers[-which(sapply(answers, is.null))])
}
saveComments <<- function() {
  answers = assembleComments()
  writeLines(answers, file(paste0('answers.',
                                 Sys.info()["user"],
                                 '.txt'))
  )
}

interactiveSection(codeToRun = "saveComments()")
```
