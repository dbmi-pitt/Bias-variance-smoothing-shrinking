---
title: "Bias, variance, smoothing, and shrinking"
resource_files:
- www/zoomAdvice.Rmd
- www/zoom_triggers.js
- www/readInnerWindow.js
- www/the-delta-method.pdf
- www/BayesEquation.Rmd
runtime: shiny
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 6
    toc_float:
      collapsed: no
      smooth_scroll: no
---


```{r setup, include=FALSE}

trackupdateDLdata = FALSE 
## change this in the shinydebugger if troubleshooting graph oscillations

knitr::opts_chunk$set(echo = FALSE)
options(knitr.duplicate.label = 'allow')
#options(error=browser)
require(T15lumpsplit)
require(shiny)
require(devtools)
if(!require(T15lumpsplit))
  devtools::install_github('professorbeautiful/T15lumpsplit')
if(!require(shinyDebuggingPanel))
  devtools::install_github('professorbeautiful/shinyDebuggingPanel')
require(magrittr);require(shinyBS)
require(knitr);require(rmarkdown);
#source('rstudio.markdownToHTML.R', local=TRUE)
```

```{r setup numbering, warning=FALSE}
try(rm(envNextNumber, pos=1), silent = TRUE)
try(rm(envNextNumberTOC, pos=1), silent = TRUE)
try(rm(envNextNumberQA, pos=1), silent = TRUE)
try(rm(envNextNumberTextQuestion, pos=1), silent = TRUE)
try(rm(envNextNumberDTC, pos=1), silent = TRUE)  ## data table
try(rm(envNextNumberTQ, pos=1), silent = TRUE)  ## data table
try(rm(envNextNumberWhoPriorProbNumericInput, pos=1), silent = TRUE)  ## data table
### Note that the .GlobalEnv seen here is different from your R session... and it persists across Rmd runs!

source('nextNumber.R', local=TRUE)
source('interactiveSection.R', local=TRUE)
source('QandAha.R', local=TRUE)
source('TextQuestion.R', local=TRUE)
source('WhoPriorProbNumericInput.R', local=TRUE)
source('conditionalPanelWithCheckbox.R', local=TRUE)
```


```{r includeFunctions}
source('inclRmd.R', local=T)
source('linkout.R', local=T)
source('interactiveSection.R', local=T)
source('includeHTMLtrimmed.R', local=T)

```


```{r initial values, echo=FALSE}

logit <<- function(p) log(p/(1-p))
antilogit <<- function(x) 1 - 1/(1+exp(x))
DLdataOriginal = matrix(c(3,3,4,90),nrow=2)
dimnames(DLdataOriginal) = list( outcome=c("R","N"), feature=c("D","L"))
ColorForPrior="orange";     
ColorForPosterior="blue";     
ColorForLikelihood="black"
TOCnum =  0
```

```{r navTOCid.R}
#source('navTOCid.R', local=T)
```


```{r servercode}
source("Plight-Pdark-posterior-new.R", local=TRUE)
source("DrWhoBayesFactor.R")
rValues = reactiveValues(tau = 1,  phi = 0.001, mu=0.5,
                         title_1='title 1',
                         DLdata = DLdataOriginal,
                         DLdataMyChoice = DLdataOriginal,
                         isResetting = FALSE,
                         isLoopingSync = FALSE,
                         WhoPriorProbProb = 1/2,
                         WhoPriorOdds = 1, #,
                         #thisSession = session
                         qStar = 1
)
```

```{r plotPlightPdarkPosteriorReactive}
plotPlightPdarkPosteriorReactive = reactive( {
  #cat("PLOTTING   ")
  tau <- input$tauInput
  phi <- input$phiInput
  mu0 <- input$mu0Input
  #cat("tau=", tau, " phi=", phi, " mu0=", mu0, "\n")
  rValues$bivariateNormResults <<-
    plotPlightPdarkPosterior(DLdata=rValues$DLdata,
                           tau=tau, phi=phi, mu0=logit(mu0), 
                           fudgeFactor = input$fudgeFactor,
                           showConfIntBinormal = TRUE
    )
  
  rValues$title_3 <<- paste0(
    "  tau=", input$tauInput,  
    ",  phi=", input$phiInput,  
    ",  mu0=", input$mu0Input 
  )
})
```


```{r lumpReact}
lumpReact = observe({
  if(length(input$lumpID) > 0) {
    #cat("lumpID\n")
    rValues$tau <- 1; rValues$phi <- 0.001   
    ### Lump:  no individual variation:   D is same as L.
    rValues$title_1 <- "Dr. Lump"
    rValues$title_2 <- HTML("Prior belief: <br>Pr(R|D) = Pr(R|L)")
  }
})  
```


```{r splitReact}
splitReact = observe({
  if(length(input$splitID) > 0) {
    #cat("splitID\n")
    rValues$tau <<- 0; rValues$phi <<- 1   
    ### Split:  D unconnected to L.
    rValues$title_1 <<- "Dr. Split"
    rValues$title_2 <<- HTML("Prior belief:<br> Pr(R|D) is unrelated to Pr(R|L).")
  }
}) 
```

```{r whoReact}
whoReact = observe({
  if(length(input$whoID) > 0) {
    #cat("whoID\n")
    ### Who:  discrete mixture of Lump and Split.
    rValues$title_1 <<- "Dr. Who: \ndiscrete mixture of Lump and Split."
    rValues$title_2 <<- HTML(paste0("Prior belief: <BR> Pr(Split)=",
                                    rValues$WhoPriorProb))
  }
}) 
```

```{r mixedReact} 
mixedReact = observe({
  if(length(input$mixedID) > 0) {
    #cat("mixedID\n")
    rValues$tau <<- 1/2; rValues$phi <<- 1/2 
    rValues$title_1 <<- HTML("Compromise: <br>lump some, split some")
    rValues$title_2 <<- HTML("Prior belief: <br> Pr(R|D) is somewhat related to Pr(R|L). ")
  }
})
```

```{r updateViews}
updateViews = observe({
  updateNumericInput(session=session, inputId="tauInput", 
                     value = rValues$tau)
  updateNumericInput(session=session, inputId="phiInput", 
                     value = rValues$phi)
})
```


```{r thePlot}
output$title_1_ID = renderUI({rValues$title_1})
output$title_2_ID = renderUI({rValues$title_2})
output$title_3_ID = renderUI({rValues$title_3})

output$thePlot = renderPlot(height=250,
                            {
                              par(mai=c(1,1,1,0.6))
                              par(mar=c(4,4,2,2) + 0.2)
                              #c(bottom, left, top, right)
                              par(pty='s')
                              plotPlightPdarkPosteriorReactive()
                            })
```



```{r}
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:sessioninitialized',
    function(event) {
    alert('shiny:sessioninitialized  2');
    Shiny.onInputChange('sessioninitialized2', 2)
});"
  )) 
)
```


```{r}
invisible(
  tags$head(tags$script(
    "$(document).on('shiny:connected', 
  function(event) {
    alert('shiny:connected  2');
    Shiny.onInputChange('shinyconnected2', 2)
  });"
  )) 
)
```


```{r makeDebuggingPanelOutput}
observeEvent(input$ctrlDpressed, {}) # just to flush the ctrl-D press.

shinyDebuggingPanel::makeDebuggingPanelOutput(
   session, toolsInitialState = FALSE, 
   condition='ctrlDpressed === true') 
```


```{r navTOCid.R.2}
#source('navTOCid.R', local=T)
```

```{r, results='asis'}
#cat("\U2B07")
# This shows you can get a fat arrow... but not in a plot.
```

```{r zoomRange}
    #### zoomAdvice ####
zoomRange <<- c(1200, 1400)
```

```{r zoomAdvice}
inclRmd('www/zoomAdvice.Rmd')
```

```{r browserAdvice}
inclRmd('www/browserAdvice.Rmd')
```

<a name='section-debugging'> </a>

```{r withDebuggingPanel}

fluidRow(
  shinyDebuggingPanel::withDebuggingPanel()
)
```

```{r}
includeScript('www/KeyHandler.js')
includeScript('navigateToId.js')   ### ESCAPE key to return.  
###The Chrome problem is NOT:
# the order of these includeScripts.
# due to navigateY. That works in JS console.

includeCSS('TOC.css')    
### includeCSS doesn't work early on.
### It worksin the YAML.
includeCSS('lumpsplit.css')
includeScript('www/jquery-3.3.1.min.js')

```


```{r panelOfInputs}
panelOfInputs = 
  wellPanel(
    #checkboxInput(inputId= 'togglePanelOfInputs', label =
      strong(em(
                    "Prior Distribution Inputs")), 
    #value = TRUE),
    #conditionalPanel(
    #  "input.togglePanelOfInputs",
    fluidRow(
      column(3, actionButton("lumpID", label = "Lump")),
      column(3, actionButton("splitID", label = "Split")),
      column(3, actionButton("mixedID", label = "Mixed")),
      column(3, numericInput(inputId = 'fudgeFactor', 
                   label = 'continuity fudge factor',
                   value=0.001))
    ),
    fluidRow(
      column(4,
             numericInput("phiInput", 
                          "prior variance | group (phi)", 
                          value=0, min = 0.00, step=0.1)),
      column(4,
             numericInput("tauInput", 
                          "shared additional variance (tau)", 
                          value=1, min = 0.00, step=0.1)
      ),
      column(4,
             numericInput("mu0Input", "shared prior mean (mu)", 
                          value=0.5, min = 0.001, step=0.1, max=0.999))
    )
  )
```


```{r show-hide-contours}
ContoursPanelLegend = list(
  div(style="color:orange", 
      checkboxInput("checkPrior", 
                    "Orange = prior distribution",
                    TRUE)),
  div(style="color:blue",
      checkboxInput("checkPosterior", 
                    "Blue = posterior distribution",
                    TRUE)),
  "Shaded: 50% highest posterior region",
#  "X:   observed data", br(),
  div(style='color:red',
      "L:  Dr.Lump's MP estimate", br(),
      "S:  Dr.Split's MP estimate", br(),
      "W:  Dr.Who's MP estimate"
  )
)
```

```{r responseRates}
output$responseRates = renderUI({
  splitLayout(HTML("Response <br>rates:"),
                HTML(paste(
                  input$mRD, '/', input$mRD+input$mND, 
                  "<br> = ",
                  signif(digits=3,
                        input$mRD/(input$mRD+input$mND)))),
                HTML(paste(
                  input$mRL, '/', input$mRL+input$mNL, 
                  "<br> = ", 
                  signif(digits=3,
                        input$mRL/(input$mRL+input$mNL))))
    )
  # paste(input$mRD, '/', input$mRD+input$mND, "for D, and ",
  #       input$mRL, '/', input$mRL+input$mNL, "for L.")
})
```

```{r data table}
source('dataTableComponent.R', local=TRUE)
```

```{r panelOfBifurcation}
output$fisherResult = renderText({
    Pvalue = fisher.test(rValues$DLdata)$p.value
    paste0(
        '   P = ', signif(digits=3, Pvalue), '\n')
})
output$fisherResultOriginal = renderText({
    Pvalue = fisher.test(DLdataOriginal)$p.value
    paste0(
        '   P = ', signif(digits=3, Pvalue), '\n')
})

```



```{r UI_begins}
require(shinyBS)
```
*Package:T15lumpsplit  `r packageVersion('T15lumpsplit')`  `r packageDate('T15lumpsplit')`     Author: Roger Day, University of Pittsburgh*


# OVERVIEW of this Module

```{r Overview}
h3("NOTE: this is a work in progress!")
conditionalPanelWithCheckbox(
  labelString = 'Overview', 
  filename = 'Overview.Rmd', 
  #html = HTML('<b>EXTRA HTML</b>'),
  # works with or without filename.
  initialValue = TRUE)
```
The general scope of the bias-variance axis in many settings is here:

```{r biasVariancePicture}
#### THIS WORKS. In the Preview window, iframe is turned into external to Preview  app. but not from the browser, that's OK. ####

#### bias-variance axis ####
conditionalPanelWithCheckboxPDF(labelString='The bias-variance axis',
     filename="Reproducibility-lump-split-page-1.pdf",
     cbStringId='cbStringIdReproducibility')

```


The list of topics developed or in development is here:


```{r topics}

conditionalPanelWithCheckbox(
  labelString = 'Topics in development', filename = 'topics.Rmd',
  html=HTML('<b>EXTRA (newline) <br>HTML</b>'), #OK  
  initialValue=FALSE)
```

## The Lump versus Split Dilemma <a name="section-LSnotebookBegin"> 

```{r submoduleTitle}
##  <a name="LSnotebookBegin"> link The Lump versus Split Dilemma </a>
hr()
#h2(style="text-align: center;", ' h2 The Lump versus Split Dilemma')
hr()
```


```{r include notebook}
#div(
#  list(fluidRow(column(width = 8, 
         # wellPanel(id = "leftScrollPanel",
                   # style = "overflow-y:scroll; max-height: 600px"
         # ,
#         inclRmd('Submodule-LumpSplit.Rmd')
 #        ),
  # column(width = 4, 
  #       wellPanel(id = "leftScrollPanel",
  #                  style = "overflow-y:scroll; max-height: 600px"
  #        , 
         #panelOfData,
         #dataTableComponent(),
         #panelOfBifurcation,
         
#)
```

```{r listings}
source('initializations.R', local=TRUE)
# ls()
# cat('-=-=-=-=\n')
# ls(pos=1)
# cat('-=-=-=-=\n')
# so it has its own .GlobalEnv, and it's persistent across knits.
# wrapperToGetKeys  ### interesting! that's mine! from DebugTools
```



```{r hide=TRUE}
require('shiny')
```

### ["Lumping" versus "splitting" in science.](#Lumping_versus_ splitting_in_science) 

In science, advances often proceed by "splitting": things that seem initially the same are classified as different. The classification may be tentativedistinction, until it proves to be useful. 
Other advances proceed by "lumping":  identifying things that seem different but have a deep connection.

So, we split birds and bats, but we lump bats and whales because they are both mammals. At least in this case, for some purposes, lumping supercedes splitting.
(But if studying modes of  locomotion, the lumping will be different.)

In medicine both splitting and lumping are important.

Cancer was just "cancer" before it was learned through clinical trials that some medicines work well on cancer in one organ but not another--- and vice versa for another medicine.

In recent years, however, specific molecular defects in cancer cells have "lumped" together types of cancer that we would not have dreamed of connecting.
The drug gleevec is a miracle drug for patients with chronic myelogenous leukemia (CML). But not for other leukemias. Deep insight into molecular biology of cancer showed that a radically different kind, gastrointestinal stromal tumor (GIST). 
Better lumping together with better splitting can cure cancer patients.

We will explore a simple lumping and splitting conundrum as a jumping off point for introducing a collection of important statistical ideas that connect to each other.

```{r QA-intro-lump-split}
QandAha(context='QA-intro-lump-split')
```

### Hypothetical medical study

The Problem:  A new treatment is given to `r sum(DLdataOriginal)` patients.  
Of them, only `r sum(DLdataOriginal["R", ])` respond.  But there is a subgroup of just `r sum(DLdataOriginal[ , "D"])` in which `r sum(DLdataOriginal["R","D"])` patients respond, yielding a response rate of `r round(100*sum(DLdataOriginal["R","D"])/sum(DLdataOriginal[ , "D"]) )`%. For now we call them "$D$ patients", and the others "$L$ patients". Which subgroup a patient belongs to we will call the person's "feature" $X_{DL}$, with possible values $D$ or $L$.
Possibilities for the actual identity of $X_{DL}$ will be explored later.

The outcome we call $Y$.  A responding patient has $Y=R$; non-reponding, $Y=N$.

Goal: decide whether to treat the "$D$ patients" (or at least, whether to put resources into studying it further for "$D$ patients").
```{r}
output$confInterval_Lump = renderText(
  {
    paste("Lump:  (", 
          signif(digits=2,
              rValues$estimates[['confInterval_Lump']][1]),
          "-",
          signif(digits=2,
              rValues$estimates[['confInterval_Lump']][2]),
          ")"
    )
  })
output$confInterval_Split = renderText(
  {
    paste("Split:  (", 
          signif(digits=2,
                 rValues$estimates[['confInterval_Split']][1]),
          "-",
          signif(digits=2,
                 rValues$estimates[['confInterval_Split']][2]),
          ")"
    )
  }) 
  estimates = reactive({
  rValues$estimates = list(
    lump = sum(rValues$DLdata['R', ])/
      sum(rValues$DLdata[ , ]),
    split = (rValues$DLdata['R', 'D'])/
      sum(rValues$DLdata[ , 'D']),
    confInterval_Split = binom.test(x = rValues$DLdata['R', 'D'],
                                n = sum(rValues$DLdata[ , 'D']) )$conf.int,
    confInterval_Lump = binom.test(x = sum(rValues$DLdata['R', ]),
                                n = sum(rValues$DLdata[ , ]) )$conf.int
  )
})
  
```


#### Lump and Split estimates

Examine the data table below. 

(Throughout this example, you can change the numbers in the table and observe the results. 
Be sure to press "reset" when done.)

If you *"lump"*, ignoring the group, you will estimate that the response rate (`Pr(R|D)`) for $D$ patients is the same as `Pr(R)`, utilizing the whole set of patients, which is `r reactive(round(100*estimates()$lump))`*%*. 

If you *"split"*, you will estimate that the response rate (`Pr(R|D)`), utilizing the D patients only, is 
`r reactive(round(100*estimates()$split))`*%*. 

In the following plot, we see the estimate and the confidence interval for `Pr(R|D)`, depending on whether the *"Lump"* or *"Split"* approach is taken. The confidence interval for *"Split"* is much wider.

```{r plotLumpSplitPoints}
a(name='section-a_plotLumpSplitPoints')
# I removed fig.height=300
output$confInterval_Lump = renderText(
  {
    try(silent = TRUE,
        paste("Lump:  (", 
          signif(digits=2,
              rValues$estimates[['confInterval_Lump']][1]),
          "-",
          signif(digits=2,
              rValues$estimates[['confInterval_Lump']][2]),
          ")"
    ) )
  })
output$confInterval_Split = renderText(
  {
    try(silent = TRUE,
        paste("Split:  (", 
          signif(digits=2,
                 rValues$estimates[['confInterval_Split']][1]),
          "-",
          signif(digits=2,
                 rValues$estimates[['confInterval_Split']][2]),
          ")"
    ) )
  }) 
output$plotLumpSplitPoints = renderPlot(
  height = 300, {
  plotPlightPdarkPosterior(DLdata=rValues$DLdata,
                           showPrior = FALSE, 
                           showPosterior=FALSE, showLikelihood=FALSE,
                           showS = TRUE,
                           showL = TRUE,
                           showW = FALSE)
  })
fluidRow(
  column(4, 
         br(),
         span(style='text-align:center;font-weight: bold', 
              "Confidence intervals:"),
         tagAppendAttributes(style="color:red;text-align:center",
                             textOutput('confInterval_Lump')),
         tagAppendAttributes(style="color:blue;text-align:center",
                             textOutput('confInterval_Split')),
         # span(style='color:red; text-align:center', 
         # "L = Lump, S = Split"),
         plotOutput('plotLumpSplitPoints')
  ),
  column(8,
         #br(),br(),br(),
         dataTableComponent())
)
# div(br(),br(),br())
```

Lumping gives an answer with low variance ($N$=100) but high bias-- because the answer reflects far more $L$ patients than $D$ patients. So we get a precise answer to what might be the wrong question. The confidence interval follows the diagonal where $Pr(R|D)=Pr(R|L)$.

Splitting gives an answer with high variance ($N$=5) but low bias-- because it is including only patients who ARE in the $D$ group:  it is directly asking the right question, but with little precision. The very wide horizontal confidence interval reflects this high variance.

#### Parametrizations of the probabilities

To proceed to deeper approaches, we need some notation.

```{r child = 'Rmd-text-snippets/parametrize.Rmd'}
```

```{r}
#includeHTML('Rmd-text-snippets/parametrize.html')
#inclRmd('Rmd-text-snippets/parametrize.Rmd')
TextQuestion('Why focus on this parameter?')
```


```{r QA-parametrization}
QandAha(context='QA-parametrization')
```

### Approaches

* [Classical test: is $X_{DL}$ independent of the outcome Y?](#ClassicalTest)
* [Bayes mixture:  "Dr. Who"](#Bayes_mixture)
* [Bayes joint prior, logit scale](#Bayes_joint_prior).  
* Approaches used in the famous ECMO data set. (ToDo)

#### <a name='section-ClassicalTest'> Classical estimation bifurcating on a hypothesis test</a>

A time-honored but fading technique is to use a classical hypothesis test, to guide which of two analyses to present.
Here the hypothesis is "$X_{DL}$ is independent of Y";  the response rate of D patients does not differ from L patients.  Then, depending on the hypothesis test result, Dr.Lump's answer or Dr. Split's answer is selected. 

One test frequently performed to test independence in a 2x2 table like this one is the *Fisher exact test*. On the original data, the P value is `r textOutput('fisherResultOriginal', inline=TRUE)`. If you have changed the data, it is now `r textOutput('fisherResult', inline=TRUE)`.

If the P-value is "significant" (which "by tradition" means "less than 5%"), the "null hypothesis" (that "Dr. Lump" is correct) is rejected. With the original data, the traditional consequence is that we use Dr. Split's estimate:  60%...  and WE COMPLETELY IGNORE THE DATA FOR THE "L" PATIENTS!

```{r QA-fisher}
QandAha('QA-fisher')
```

#### <a name='section-Bayes_mixture'> Approach: Dr. Who's Bayes mixture of "Lump" and "Split"</a>

Dr. Who doesn't know *who* to believe, but is prepared to let the data make the right compromise.


```{r child = 'DrWho.Rmd'}
```

```{r QA-DrWho}
QandAha('QA-DrWho')
```

####  Approach: Optimizing the mixture with cross-validation

Cross-validation is a technique in which the performance of a model can be checked as if on an independent dataset, without actually having a separate dataset. *$K$-fold cross-validation* divides the dataset into K disjoint subsets of observations, and loops over these sets. Each subset is, in turn, used as a `test set`. We remove it from the full dataset, re-build the model on the reduced dataset (the `training set`), and evaluate predictions of that model on the removed observations in the test set. Performance metrics are usually averaged across the $K$ repetitions. When $K=N$, the size of the full dataset, the procedure is called `leave-one-out cross-validation`, since each test set is of size one.

For this example dataset, we perform leave-one-out cross-validation as follows:

```{r}
source('Lump-Split-crossvalidation.R', local=TRUE, echo = FALSE)
```

```{r}
a(name='section-a_crossvalidationPlot')
fluidRow(
  column(4, 
         plotOutput('crossvalidationPlot')),
  column(8,
         br(),
         dataTableComponent())
)
```

```{r TQ and QA for crossvalidation}
TextQuestion("What advantage does the cross-validation approach here have, relative to the approaches of Drs. Split, Lump and Who?")
QandAha('QA for crossvalidation')
```

#### <a name='section-Bayes_joint_prior'> Approach: Bayes joint prior, logit scale.</a>

Another Bayesian approach is to set a Bayesian prior on the joint distribution of the two conditional probabilities Pr(R|D) and Pr(R|L). This allows data on the two probabilities to be shared between them, smoothly instead of a crude mixture of the two extreme views of   "Dr. Lump" and "Dr. Split".
```{r QA-smooth-joint-prior}
QandAha('QA-smooth-joint-prior')
```
Once we have this joint prior distribution, we compute the joint posterior.

To set the prior on the joint distribution of Pr(R|D) and Pr(R|L), we first convert them to logits ("logit" means "log(odds)").
```{r}
conditionalPanelWithCheckbox('Details about the logit', filename = 'www/logit.Rmd')
```


Then we set a bivariate normal distribution on the logits.

We also convert the observed proportions to logits. We estimate the variances using the delta method. Details are here:
```{r}
conditionalPanelWithCheckboxPDF(
  labelString="The delta method", 
  filename='www/the-delta-method.pdf',
  cbStringId='delta_method')
```

We can apply the delta method to the Poisson distribution, useful for count data like the data in our table. The variance of the logarithm of a count is roughly the reciprocal of the count.

(This doesn't work well if the count is near zero, and not at all at zero.
For this reason a "continuity fudge" is applied, if necessary.)

```{r}

output$postmean.p = renderText(
  paste0(
    signif(digits=2, rValues$bivariateNormResults$postmean.p[1]),
    ' (95% confidence interval: ', 
    signif(digits=2, rValues$bivariateNormResults$confints.p[1 , 1]),
    '-',
    signif(digits=2, rValues$bivariateNormResults$confints.p[2 , 1]),
    ')'
  )
)
```

Finally, Pr(R|D) marginalized over Pr(R|L) has posterior mean =  `r  (textOutput('postmean.p', inline=TRUE))` .

```{r linkoutbivariate}
conditionalPanelWithCheckboxPDF(
  labelString='Derivation of the bivariate normal posterior distribution', 
  filename='./lump,split-bivariate-normal-derivation.pdf', 
  cbStringId = 'bivnormPDF')

# observeEvent(eventExpr = {input$bivnorm},
#              { 
#                  linkout(
#                    './lump,split-bivariate-normal-derivation.pdf') 
#              }
# )
```

WILL DELETE THIS: The link to bivariate normal details is 
`r actionLink('bivnorm', label=HTML('<font color=blue>here.</font>'))`

```{r QA-joint-posterior}
TextQuestion("Your reactions to the derivation?")
QandAha('QA-joint-posterior')
```


```{r BayesLogitPlot}
a(name='section-a_viewPlot')
div(
  fluidRow(
    column(4, 
           tagAppendAttributes(
             div(
               uiOutput(outputId="title_1_ID"),
               uiOutput(outputId="title_2_ID"),
               uiOutput(outputId="title_3_ID")
             ),
             style=
               'text-align:center; font-size:small;
                    font-weight:bold'),
           plotOutput(outputId = 'thePlot', height=260)
    ),
    column(8,
           br(),
           dataTableComponent()
    )
  ),
  fluidRow(
    column(4, 
           ContoursPanelLegend
    ),
    column(8,
           panelOfInputs
    )
  )
)
```


#### Summary table of estimates 

Assembling the results of the approaches to estimating Pr(R|D):

```{r}
tableOfEstimates = reactive({
  nRD=rValues$DLdata['R','D']
  nND=rValues$DLdata['N','D']
  posterior.mean.s = (1+nRD)/(2+nRD+nND)
  nR=sum(rValues$DLdata['R', ])
  nN=sum(rValues$DLdata['N', ])
  posterior.mean.l = (2+nR)/(4+nN)
  
  pProbSplit = posteriorProb(theData=rValues$DLdata)
  pProbLump = (1-pProbSplit)
  posterior.mean.w = 
    pProbSplit * posterior.mean.s +
    pProbLump * posterior.mean.l

  LumpRawMean=(nR)/(nN)
  SplitRawMean=(nRD)/(nRD+nND)
    result = data.frame(
      LumpObs=LumpRawMean,
      SplitObs=SplitRawMean,
      Lump=posterior.mean.l,
      Split=posterior.mean.s ,
      PrLump=pProbLump,
      PrSplit=pProbSplit,
      Who=posterior.mean.w #, 
      #CV=rValues$CVoptimalEstimate,
      #BayesLogit=rValues$bivariateNormResults$postmean.p[1]
    )
    round(result, digits=2)
})
output$estimateTable = 
  renderTable({
    ## TODO: use the conditional posterior means instead.
    ##  i.e. add in the pseudocounts.
    tableOfEstimates()
})
tableOutput('estimateTable')
```

As you change Dr. Who's prior probability or the data values, observe the changes in this summary table.

```{r QA-Who-prior-probability}
QandAha('QA-Who-prior-probability')
```


##	The role of external information, explored by introducing different identities for "D" and "L".

So, when should we lump? When should we split? Should we aim to compromise, and if so, how should we strike the balance?  <a node='Lumping_versus_ splitting_in_science'> Lumping and splitting in science </a>

In this section we explore different scenarios of external information, to see how they affect the estimation of ${Pr(R|D}$.

###	Strong prior belief that feature $X_{DL}$ is *un*important

What if D and L represent dark hair color and light hair color? "Dr. Who" may be highly skeptical that this $X_{DL}$ really affects response, and express this by choosing a prior odds that favors Dr. Lump  over Dr. Split. Experiment by choosing such a value for prior odds here: 

```{r }
WhoPriorProbNumericInput()
```

and reviewing revised results at these previous locations:


```{r QA-Strong-prior-belief-X_DL-unimportant}
inclRmd('jumpBack.Rmd')
QandAha('QA-Strong-prior-belief-X_DL-unimportant')
```


###	Strong prior belief that the feature $X_{DL}$ is important

What if D and L represent two alleles of a gene believed to affect this drug's pharmacodynamics? Or perhaps, $X_{DL} = D = dark hair$, $X_{DL} = L = light hair$, but hair color is strongly tied to ethnicity in this sample … which is strongly tied to a key genetic variant.

"Dr. Who" can express this by choosing a prior odds that favors Dr. Split over Dr. Lump. Experiment by choosing such a value here: 

```{r}
WhoPriorProbNumericInput()
```

and reviewing revised results at these previous locations:

  
```{r QA-Strong-prior-belief-X_DL-important}
inclRmd('jumpBack.Rmd')
QandAha('QA-Strong-prior-belief-X_DL-important')
```


###	Multiple features, with some prior belief

Suppose that we have features for a hundred genes which were *previously* identified as known to affect the pharmacodynamics of the drug being tested. Our feature $X_{DL}$ happens to be one of them; otherwise unselected.

```{r}
TextQuestion(' How would you modify the "classical" approach that chooses Lump or Split based on a hypothesis test?')
```


```{r}
TextQuestion(' How would you account for this knowledge in one of the two Bayesian approaches?')
```

###	Best feature of 100, with some prior belief

As before, suppose that we have features for a hundred genes which were *previously* identified as known to affect the pharmacodynamics of the drug being tested. But this time, our feature $X_{DL}$ is selected to be the best of the one hundred features.

This situation entails dealing with *"multiple comparisons"*. Pure chance can make a feature seem strongly associated with the response outcome when in reality it is not associated. When the best-appearing relationship is selected out of a large number, we can be fooled into thinking the strength of evidence is stronger than it really is.

*"The more questions you ask, the more wrong answers you are likely to get."*
```{r}
conditionalPanelWithCheckboxPDF(
  labelString="Intro to multiple comparisons", 
  filename='Multiple comparisons- introduction.pdf',
  cbStringId='multiple_comparisons')
```


One way to handle multiple comparisons is through a classical frequentist multiple testing adjustment. This evaluates the chance of getting a P-value as low as (or lower than) the minimum of the P-values for the features. The best-known of these methods are the Bonferroni, Sidak, and permutation /randomization tests. Here are some details on these approaches: 
```{r}
conditionalPanelWithCheckboxPDF(
  labelString="Multiple comparisons- classic methods", 
  filename='Multiple comparisons- classic methods.pdf',
  cbStringId='Multiple_comparisons_classic')
```

and here are some examples that highlight some strange and unsettling aspects of the classical methods:

```{r}
conditionalPanelWithCheckboxPDF(
  labelString="Multiple comparisons- disturbing examples", 
  filename='Multiple comparisons- disturbing examples.pdf',
  cbStringId='Multiple_comparisons_disturbing')
```

Let's put those worries aside for now.

Let's generate 99 extra features and examine how the methods behave when applied when the extra features are added to the D/L feature of our little data set. You can use either the original data or the data as you may have modified it.

```{r}
# makeDLdataDFYesNoYes = function(DLdata, qStar=1){
#   #  OK, mission accomplished.
#   isYesNoYes = FALSE
#   seedInt = 44
#   #assign('.Random.seed', readRDS('savedSeed.rda') )
#   print(.Random.seed[626])
#   while(! isYesNoYes) {
#     #savedSeed = .Random.seed
#     seedInt = seedInt + 1
#     set.seed(seedInt)
#     DLdataDFResult = makeDLdataDF(DLdata = DLdata)
#     BHtable = DLdataDFResult$BHtable
#     isYesNoYes = identical(
#       as.vector(BHtable['P<cutoff', 1:3]),
#                              c('yes', 'no', 'yes') )
#     cat(0+isYesNoYes, seedInt, '\n')
#   }
#   print(BHtable[ , 1:4])
#   saveRDS(seedInt, 'seedInt.rda')
#   return(seedInt)
# }

makeDLdataDF = function(DLdata, qStar=1) {
  set.seed(readRDS('seedInt.rda'))
  DLdataDF = 
    data.frame(
      outcome = rep(c("R","N", "R","N"),
                    times=c(DLdata)),
      X_DL = rep(c("D","D", "L","L"),
                 times=c(DLdata))
    )
  NextraFeatures = 99
  extraFeaturesProbs_R = rbeta(NextraFeatures, 10, 10)
  extraFeaturesOdds_R =
    extraFeaturesProbs_R/(1-extraFeaturesProbs_R)
  #extraFeaturesProbs1 = rbeta(NextraFeatures, 10, 10)
  tauTrue = 1
  oddsratios = exp(rnorm(NextraFeatures, 0, sqrt(tauTrue)))
  extraFeaturesOdds_N = extraFeaturesOdds_R * oddsratios
  extraFeaturesProbs_N =
    extraFeaturesOdds_N/(1+extraFeaturesOdds_N)
  #plot(extraFeaturesProbs_N, extraFeaturesProbs_R)
  extraFeatures_N = sapply(extraFeaturesProbs_N,
           rbinom, n=sum(DLdataDF$outcome=='N'),size=1)
  extraFeatures_R = sapply(extraFeaturesProbs_R,
           rbinom, n=sum(DLdataDF$outcome=='R'),size=1)
  extraFeatures = 
    matrix(NA, ncol=NextraFeatures, nrow=nrow(DLdataDF))
  extraFeatures[DLdataDF$outcome=='N', ] = extraFeatures_N
  extraFeatures[DLdataDF$outcome=='R', ] = extraFeatures_R
  colnames(extraFeatures) = 
    paste0('X', 1+(1:NextraFeatures))
  DLdataDF = cbind(DLdataDF, extraFeatures)
  allFisherPs <<- 
    sapply(DLdataDF[-1], function(feature) 
      fisher.test(feature,
                  DLdataDF$outcome)$p.value
    )
  options.saved = options(warn=-1)
  allChisqPs <<- 
    sapply(DLdataDF[-1], function(feature) 
      chisq.test(feature,
                 DLdataDF$outcome)$p.value
    ) 
  options(options.saved)
  BHtable = makeBHtable(allChisqPs, qStar=qStar)
  return(invisible(
    list(DLdataDF=DLdataDF, allFisherPs=allFisherPs,
         allChisqPs=allChisqPs, BHtable=BHtable))
  ) 
}
makeBHtable = function(pValues, qStar=1, columns=8) {
  somePvalues  = 
    signif(digits=3, sort(pValues)[1:columns])
  BHtable = data.frame(
    Pvalue = somePvalues,
    cutoff = qStar * (1:columns)/100)
  BHtable[[ 'P<cutoff' ]] = c('no','yes')[
    1 + (BHtable$cutoff > BHtable$Pvalue) ]
  BHtable = t(BHtable)
  rownames(BHtable) = c('Pvalue', 'cutoff', 'P<cutoff')
  return(BHtable)
}
DLdataDF_originalResult = makeDLdataDF(DLdataOriginal)
allFisherPs_original = DLdataDF_originalResult$allFisherPs
allChisqPs_original = DLdataDF_originalResult$allChisqPs
BHtable_original = DLdataDF_originalResult$BHtable
```

```{r}
actionButton(inputId = 'regenerateFeatures', label = 'regenerate Features')
observeEvent(
  list(input$regenerateFeatures, 
       rValues$DLdata, rValues$qStar),
  {
    makeDLdataDFResult = makeDLdataDF(
      DLdata = rValues$DLdata, qStar=rValues$qStar)
    rValues$DLdataDF = makeDLdataDFResult$DLdataDF
    rValues$allFisherPs = makeDLdataDFResult$allFisherPs
    rValues$allChisqPs = makeDLdataDFResult$allChisqPs
    allFisherPs <<- rValues$allFisherPs
    allChisqPs <<- rValues$allChisqPs
    rValues$BHtable = makeDLdataDFResult$BHtable
  })

```

Now that we have a data set, let's do Fisher exact tests and chisquare tests to all the features. The red "X" is of course our original data feature, with $X_{DL}$ = D or L.

```{r}
output$pvaluesPlots = renderPlot({
  hist(rValues$allFisherPs)
  #head(sort(allFisherPs))
  hist(rValues$allChisqPs)
  #head(sort(allChisqPs))
  plot(rValues$allChisqPs, rValues$allFisherPs, log='xy')
  points(rValues$allChisqPs[1], rValues$allFisherPs[1],
         pch='X', col='red')
} )
plotOutput('pvaluesPlots')

```

Applying the *"Bonferroni correction"* to the Fisher P value for our feature $X_{DL}$ means multiplying the P value `r signif(digits=4, allFisherPs_original[1])` by 100, to get P=`r signif(digits=4, 100* allFisherPs_original[1])`. So the evidence that the `D` people really are different is minimal at best. (To say it carefully, if a 'cutoff' for defining 'statistically significant' happened to be chosen at `r signif(digits=4, 100* allFisherPs_original[1])`, then our result for $X_{DL}$ would be just on the edge of 'significance'. That's not a compelling cutoff!) 

In the classical statistics spirit, we would now go with Dr. Lump instead of Dr. Split. (Choosing instead the chisquare test, the Bonferroni correction gives us P=`r signif(digits=4, 100* allChisqPs_original[1])`).

As before, you have the opportunity to play around with changing the data, or reverting to the original data set.

`r dataTableComponent('hide')`

```{r QA-multiple-comparisons-corrections}
QandAha('QA-multiple-comparisons-corrections')
```

### The Benjamini-Hochberg method

Another classical approach, the Benjamini-Hochberg method, is widely used in high-throughput biological assays. It is also classical frequentist method. It has a technical advantage in that the stated "false discovery rate" is accurate. However, it also has peculiar and disturbing behavior:

```{r}
conditionalPanelWithCheckboxPDF(
  labelString="The Benjamini-Hochberg method", 
  filename='Benjamini-Hochberg.pdf',
  cbStringId='Benjamini_Hochberg')
```

When applied to our augmented data set with 100 potential predictors, this is what we get, depending on qStar, the upper bound for the false discovery rate (FDR):

```{r}
numericInput('qStarID', 'qStar', 
             value = 1, min=0,  step=0.1)
observeEvent(input$qstarID, {rValues$qStar = input$qStarID})

output$BHtable = renderTable({
  qStar = rValues$qStar
  options.saved = options(digits=3)
  rValues$BHtable
})

tableOutput('BHtable')
```



```{r}
TextQuestion("Using the original data, and the FDR upper bound (qStar) equal to one, which features are 'significant'? How many 'false discoveries' are there?")
```



```{r}
TextQuestion("Raising qStar from 1.0, how does the collection of 'significant' features change?")
```

This is a good time to record your thoughts!

```{r QA-Benjamini-Hochberg}
QandAha('QA-Benjamini-Hochberg')
```

### An Empirical Bayes approach to multiple comparisons

Another approach is through *empirical Bayes*, a method that presumes that the associations of the features with the outcome are drawn from a distribution. This allows a kind of sharing of information reminiscent of the sharing of information between the $X_{DL}$=D and $X_{DL}$=L patients, but sharing across features instead of across subgroups defined by one feature. 

The following link develops the empirical Bayes idea in the context of medical decision-making. When done reading this, close the panel, record your Questions and Aha's, and continue to see the application of EB for our example.

```{r}
conditionalPanelWithCheckboxPDF(
  labelString="An Introduction to Hierarchical Bayes and Empirical Bayes", 
  filename='www/Intro_to_Hierarchical_Bayes_and_Empirical_Bayes.pdf',
  cbStringId='EmpiricalBayes')
```

We can apply EB to a collection of estimates. In this instance, let's collect the estimates of association between the outcome and all 100 features. These estimates are the odds ratios. For the original feature $X_{DL}$, the odds ratio is
`r DLdataOriginal['R','D']` / `r DLdataOriginal['N','D']` * `r DLdataOriginal['R','L']` / `r DLdataOriginal['N','L']`), which is 
`r oddRatioOriginal = DLdataOriginal['R','D'] / DLdataOriginal['N','D'] * DLdataOriginal['R','L'] / DLdataOriginal['N','L'] ` ).

This can be compared to all 100 odds ratios for the features:

```{r}
output$oddsratioplot = renderPlot({
  outcome = rValues$DLdataDF$outcome
  oddsratio = function(col, sprinkle=1) {
    theTable = table(col, outcome)
    theTable = sprinkle + theTable
    return(theTable[1,1]/theTable[2,1]
           /theTable[1,2]*theTable[2,2])
  }
  oddsratios = sapply(rValues$DLdataDF[-(1)], oddsratio)
  rValues$logoddsratios = 
    logoddsratios = log(oddsratios)
  plot(density(logoddsratios))
  logoddsratiovariance = function(col, sprinkle=0.1) {
    theTable = table(col, outcome)
    theTable = sprinkle + theTable
    sum(1/theTable)
  }
  rValues$logoddsratiovariances =
    logoddsratiovariances =
    sapply(rValues$DLdataDF[-(1)],
                                 logoddsratiovariance)
  points(logoddsratios, col='red', type='h',
       logoddsratiovariances * par('usr')[4]/
        max(logoddsratiovariances) *0.3)
  points(logoddsratios[1], 0, pch="X", col='green')
})
plotOutput('oddsratioplot')
```
Using the delta method mentioned above,  have variances roughly equal to the sums of reciprocals of the counts. From this we can construct confidence intervals for the logits, then pull them back to confidence intervals for the odds ratios. So for example, for our original $X_{DL}$=D or L feature, the confidence interval is given by
`exp ( log(oddRatioOriginal) + c(-1,1)*sqrt(sum(1/DLdataOriginal))*qnorm(0.975))`.

To apply the EB idea, we use the data for all the features, to "shrink" all the logit ratios ${X_i}$ towards the common mean, estimated as ${\hat\mu}$. This solution uses a model in which the true logit ratios are all drawn from a common parent distribution, assumed to be normal (Gaussian). Then a standard Bayesian calculation gives us the posterior means (and variances) for each logit ratio. In particular, letting ${\mu_i}$ be the logit ratio for feature $i$, with variance ${\sigma_i}$ (which we estimate from the delta method to get ${\hat\sigma_i}$), the posterior mean is 

  $E({\mu _i}|{X_i}) = {X_i}(1 - {w_i}) + \hat \mu {w_i}$,
 
 where the weights $w_i$ vary with the feature:
 
  $w_i = \frac{{\hat\sigma_i}}{{\hat\sigma_i}+\hat{A}}$ .
  
$\hat{A}$ is the estimated prior variance for all the true ${\mu_i}$.
 
This is yet another example of bias-variance trade-off. By accepting some bias in including data from other features, the variance of the estimate decreases, enough to make the net result more accurate.
 
But we would have to know the true mean and variance of the parent distribution. 

The "empirical" part of empirical Bayes is to use the data to estimate the mean and variance of the parent distribution generating the $\mu_i$. As you can see in the document in the panel just above, the "empirical" step needs the marginal distribution of the observed logit ratios.

  ${X_i}|\tau \sim N(\mu,\tau  + {\sigma _i})$

where we will get the variances ${\sigma _i}$ from the delta method. The unknown $\mu$ and $\tau$ are called the "hyperparameters". We can estimate them however we prefer, but using maximum posterior estimation we get

$\hat \mu  = \sum {\left( {{X_i}{w_i}} \right)} /\sum {\left( {{w_i}} \right)}$

where the weights are ${w_i} = 1/{\sigma _i}$. 
These are the optimal weights for smallest variance of $\hat \mu$. 

As an *optional topic*, you may read the following on Lagrange multiplier method for optimization. Example (A) covers this case.

```{r}
conditionalPanelWithCheckboxPDF(
  labelString="Lagrange multipliers", 
  filename='www/Lagrange multipliers with examples.pdf',
  cbStringId='Lagrange_multipliers')
```

Continuing the "empirical" part of EB, one estimate of $\tau$ is to take the total sums of squares and subtract the portion purely due to the individual data noise of the features. When the individual variances vary greatly, the estimate is a little more complicated. One way is via maximum likelihood. 

```{r empBayeslogitRatios}
output$empBayeslogitRatios = renderPlot({
  logitRatios = rValues$logoddsratios
  logitVars = rValues$logoddsratiovariances
  print(summary(logitRatios))
  print(summary(logitVars))
  loglikFun=function(mu,tau) {
    sum(log(dnorm(logitRatios, mu, sqrt(tau+logitVars))) )
  }
  mu = mean(logitRatios)
  tau = 0   #(var(logitRatios) - mean(logitVars))
  for( irep in 1:13) { # we ain't buildin' no cathedral here.
    mu = optimize(f = loglikFun, tau = tau,
                  interval = c(-4,4), maximum=TRUE
    )$maximum
    tau = optimize(f = loglikFun, mu = mu,
                   interval = c(0, 1), maximum=TRUE
    )$maximum
  }
  cat('mu-hat: ', mu, '  tau-hat: ', tau, '\n')
  weights = tau / (rValues$logoddsratiovariances+ tau)
  rValues$posteriorMeans = 
    logitRatios * (1-weights) + mu * weights
  plot(c(-4, 3), 0:1, col='white',
       xlab= '', ylab='', axes=F,
       cex=2)
  axis(1, cex=2)
  #  axis(2, at = 0:1, labels = c("raw", "EB shrunk"), lwd = 0)
  # text(x = range(logitRatios)*1.05, y=c(0,0), 
  #      labels = c("raw", "raw"), xpd=NA, cex=2)
  # text(x = range(logitRatios)*1.05, y=c(1,1), 
  #      labels = c("EB\nshrunk", "EB\nshrunk"), xpd=NA, cex=2)
  axis(3, cex=2)
  # title(
  mtext(text = 'log of odds ratios: raw estimates ("Dr. Split" style', 
        side = 1, line = 3, cex=2, xpd=NA)
  mtext(text = 'log of odds ratios: "shrunken" estimates with empirical Bayes', 
        side = 3, line = 2.5, cex=2, xpd=NA)
  for(iobs in 2:length(logitRatios))
    lines(c(logitRatios[iobs], rValues$posteriorMeans[iobs]),
          0:1)
  lines(c(logitRatios[1], rValues$posteriorMeans[1]),
        0:1, col='green', lwd=3)
  points(logitRatios[1], 0, 
         pch="X", col='green', cex=3)
  points(rValues$posteriorMeans[1], 1,
         pch="X", col='green', cex=3)
  lines(c(logitRatios[iobs], rValues$posteriorMeans[iobs]),
        0:1)
  posteriorVariance_DL =  weights[1] * logitVars[1]
  posteriorStDev_DL = sqrt(posteriorVariance_DL)
  alpha = 0.1
  lines(logitRatios[1]+c(-1,1)*qnorm(alpha)*
          sqrt(logitVars[1]), 
        c(0,0), col='green', lwd=3)
  lines(rValues$posteriorMeans[1]+c(-1,1)*qnorm(alpha)*
          posteriorStDev_DL, 
        c(1,1), col='green', lwd=3)
})

plotOutput('empBayeslogitRatios')
```

The green horizontal lines are the 80% confidence intervals, before (below) and after (above) applying the empirical Bayes shrinking towards the mean of all the logit ratios. We see that the one above is narrower. So shrinking (which shares info among the features) makes the estimate for the log odds ratio more precise, compared to the "Dr. Split" perspective that directs us to allow only the original "D/L" feature to inform the odds ratio for that feature.



```{r}
TextQuestion(
'Is this approach really an improvement here? Is this  sharing information among the features justified? What factors might make you more, or less, inclined to take this approach? ')
QandAha()
```
This is fine as far as it goes, but our result for the odds ratio does not directly address the original question of interest:  "What is the true response rate for `D` people?" 
So here is a challenge question:

```{r}
TextQuestion('
How could we convert this estimate of the odds ratio for the X_DL feature into an estimate for Pr(Response | X_DL = D)? 
')
```


###  The Qvalue approach for multiple testing

We can also apply EB in quite a different way, to judge a collection of P values. One popular alternative to the Bonferroni and Sidak adjustments with an empirical Bayes flavor is the "Qvalue" method. 

This method assumes that the distribution of the observed P values is a mixture of a uniform distribution for the true null hypotheses and a distribution concentrated near zero for the true alternative hypotheses. The false discovery rate (FDR) and posterior probability of each null hypothesis come directly from Bayes Theorem. However, the mixture proportion, which is the prior probability of each null hypothesis, is unknown. The empirical Bayes idea directs us to estimate it. Here are some details.

```{r}
conditionalPanelWithCheckboxPDF(
  labelString="The Storey-Tibshirani Qvalue method", 
  filename='Storey-Tibshirani.pdf',
  cbStringId='Storey_Tibshirani'
)
```

Here we apply the qValue method to the chisquare P values from our dataset with 100 features:

```{r}
output$qvaluePlot = renderPlot({
  qout = rValues$qValueObject =
    qvalue::qvalue(rValues$allChisqPs)
  rValues$P_Q_for_X_DL = 
    c(qout$pvalues[1], qout$qvalues[1])
  plot(qout$pvalues, qout$qvalues, 
       xlim=c(0,0.004), ylim = c(0, 0.04))
  points(qout$pvalues[1], qout$qvalues[1], 
         col='green')
  points(qout$pvalues[1], qout$qvalues[1], 
         pch='X', col='green')
  text(qout$pvalues[1], qout$qvalues[1], adj=-0.2,
       labels = expression('\U2B05 X_DL'), col='green')
  # ⬅︎
  # LEFTWARDS BLACK ARROW
  # Unicode: U+2B05 U+FE0E, UTF-8: E2 AC 85 EF B8 8E
  abline(a=0, b=100, col='darkred')
  text(0.0005, 0.03, 'Bonferroni', srt=80, col='darkred')
  abline(a=0, b=1, col='blue')
  text(0.0025, 0.004, 'diagonal', srt=6, col='blue')
  #plot(qvalue::qvalue(rValues$allChisqPs) )
})
plotOutput('qvaluePlot')
```

From this we see that the Q-value for our original D/L feature is `r output$qv = renderText({rValues$P_Q_for_X_DL[2]}); textOutput('qv')`.



We use the package `qvalue` for this calculation, which can be obtained from https://www.bioconductor.org/packages/release/bioc/html/qvalue.html. 

```{r QA-empirical-Bayes}
QandAha('QA-empirical-Bayes')
```
### Two alleles of one gene out of a hundred thousand tested; nothing known

We won't illustrate this with data here, but think about this situation and how to approach it.

**One key point**: none of these methods utilizes *prior belief* about the *plausibility* of any or all of these features. To utilize prior belief across ALL the features, one has to develop a *joint* prior distribution across all hypotheses. This is difficult, but there has been progress.

```{r QA-big-joint-priors}
QandAha(context='QA-big-joint-priors')
```

###  Save your Comments and Answers
```{r assembleComments}

### TODO:  On startup, load the existing file into the answer boxes.
assembleQAComments = reactive( {
  lastQANumber = length(QA_contexts)
  QAComments = sapply(1:lastQANumber, function(num) {
    outputIdThisQA = paste0('QA', num)
    textareaIdThisQA = paste0('id', outputIdThisQA)
    return(input[[textareaIdThisQA]])
  })
  names(QAComments) = QA_contexts
  QAComments
})

assembleTQAnswers = reactive({
  lastTQNumber = length(TQ_contexts)
  TQanswers = lapply(1:lastTQNumber, function(num) {
    outputIdThisTQ = paste0('TQ', num)
    textareaIdThisTQ = paste0('id', outputIdThisTQ)
    return(input[[textareaIdThisTQ]])
  })
  names(TQanswers) = TQ_contexts
  TQanswers
})

assembledContentsToHTML = function(contents) {
  # result = sapply(contents, function(content){
  #   return(paste(content, '<br>'))
  # })
  result = paste(contents, collapse="<BR>")
  print(str(result))
  return(HTML(result))
}

output$commentsToText = renderUI({
  comments = assembleQAComments()  ### QA
  commentsText <<- capture.output(comments)
  #writeLines(commentsText, commentsTextFile)
  assembledContentsToHTML(commentsText)
})
output$answersToText = renderUI({
  answers = assembleTQAnswers()   ## TQ
  answersText <<- capture.output(answers)
  #writeLines(answersText, answersTextFile)
  assembledContentsToHTML(answersText) 
})
```

```{r downloadHandlers}
output$downloadAnswers <- downloadHandler(
  filename = "_answers_T15lumpsplit.txt",
  content = function(file) {
    answers = assembleTQAnswers()   ## TQ
    answersText <<- capture.output(answers)
    writeLines(answersText, file)
  }
  #, outputArgs=list()
)
output$downloadQandA <- downloadHandler(
  filename = "_comments_T15lumpsplit.txt",
  content = function(file) {
    comments = assembleQAComments()  ### QA
    commentsText <<- capture.output(comments)
    writeLines(commentsText, file)
  }
)
```

```{r assemble comments and answers}

div(
  tags$b('View and download your "Questions&Ahas" comments and "TextQuestion" answers here.'),
  downloadButton(outputId="downloadQandA", 
                 "Download your Q&A comments"),
  conditionalPanelWithCheckbox(
    labelString = 'View your comments', 
    html=uiOutput('commentsToText'),
    initialValue = FALSE  ),
  downloadButton(outputId="downloadAnswers", 
                 "Download your answers"),
  conditionalPanelWithCheckbox(
    labelString = 'View your answers', 
    #filename='www/empty.Rmd',
    html=uiOutput('answersToText'),
    initialValue = FALSE  ),
  tags$b(
    'Save these 2 files, to share & discuss with a mentor.')
)
```

# Other topics to be developed:
##	The famous ECMO data set.
##	Curse of dimensionality,
##	Multiple testing and estimation with high-dimensional priors,


```{r include navTOCid}
#source('navTOCid.R', local=T)
```

```{r}
includeScript('www/browserDetect.js')
```

